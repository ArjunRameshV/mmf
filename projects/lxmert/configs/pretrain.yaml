includes:
- ./defaults.yaml

model_config:
  lxmert:
    bert_model_name: bert-base-uncased
    training_head_type: pretraining
    random_initialize: false
    num_labels: 3129
    gqa_labels: 1534
    num_hidden_layers : 12
    num_attention_heads : 12
    intermediate_size : 3072
    hidden_size : 768
    hidden_act : gelu
    hidden_dropout_prob : 0.1
    attention_probs_dropout_prob : 0.1
    max_position_embeddings : 512
    type_vocab_size : 2
    initializer_range : 0.02
    pad_token_id : 0
    layer_norm_eps : 1e-12
    mode: 'lxr'
    l_layers: 9  # 12
    x_layers: 5  # 5
    r_layers: 5  # 0
    visual_feat_dim: 2048
    visual_pos_dim: 4
    task_matched: true
    task_mask_lm: true
    task_obj_predict: true
    task_qa: true
    visual_losses:
    - obj
#     - attr
    - feat
    visual_loss_config:
      obj:
      - 1600
      - ce
      - [-1,]
      - 6.67
      attr:
      - 400
      - ce
      - [-1,]
      - 6.67
      feat:
      - 2048
      - l2
      - [-1, 2048]
      - 6.67
    special_visual_initialize: true # i dont know what this is
    hard_cap_seq_len: 36
    cut_first: text
    embedding_strategy: plain
    bypass_transformer: false
    output_attentions: false # need to implement
    output_hidden_states: false # need to implement
    text_only: false
    freeze_base: false
    finetune_lr_multiplier: 1
    vocab_size: 30522
    fast_mode: false
    dynamic_attention: false #  need to implement
    in_batch_pairs: false
    visualization: false # need to implement
    model: "bert"


evaluation:
  metrics:
  - vqa_accuracy

optimizer:
  type: adam_w
  params:
    lr: 1e-4
    eps: 1e-8

training:
  seed: 9595
  batch_size: 64
  lr_scheduler: false
  find_unused_parameters: true
  use_warmup: true
  warmup_factor: 0.05
  warmup_iterations: 1000
  max_epochs: 20
  max_updates: null
  pin_memory: false

dataset_config:
  masked_vqa2:
    data_dir: ${env.data_dir}/datasets
    zoo_requirements:
    - coco.defaults
    - vqa2.defaults
    add_answer: true
    use_images: false
    use_features: true
    return_features_info: true
    use_image_feature_masks: true
    max_features: 36
    use_ocr: false
    use_ocr_info: false
    features:
      train:
      - coco/defaults/features/trainval2014.lmdb
      - coco/defaults/features/trainval2014.lmdb
    annotations:
      train:
      - vqa2/defaults/annotations/imdb_train2014.npy
      - vqa2/defaults/annotations/imdb_val2014.npy
    processors:
      answer_processor:
        type: vqa_answer
        params:
          num_answers: 10
          vocab_file: vqa2/defaults/extras/vocabs/answers_vqa.txt
          preprocessor:
            type: simple_word
            params: {}
      text_processor:
        type: bert_tokenizer
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          max_seq_length: 20
      masked_token_processor:
        type: masked_token
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          mask_probability: 0.15
          max_seq_length: 20
      masked_region_processor:
        type: masked_region
        params:
          mask_probability: 0.15
          mask_region_probability: 0.9
      bbox_processor:
        type: bbox
        params:
          max_length: 36
      attribute_processor:
        type: vocab
        params:
          max_length: 1
          vocab:
            type: random
            vocab_file: vqa2/defaults/extras/vocabs/vocabulary_100k.txt
  masked_gqa:
    data_dir: ${env.data_dir}/datasets
    zoo_requirements:
    - gqa.defaults
    add_answer: true
    use_images: false
    use_features: true
    return_features_info: true
    use_image_feature_masks: true
    max_features: 36
    use_ocr: false
    use_ocr_info: false
    features:
      train:
      - gqa/defaults/features/gqa.lmdb
      - gqa/defaults/features/gqa.lmdb
    annotations:
      train:
      - gqa/defaults/annotations/train_balanced_questions.npy
      - gqa/defaults/annotations/val_balanced_questions.npy
    processors:
      text_processor:
        type: bert_tokenizer
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          max_seq_length: 20
      masked_token_processor:
        type: masked_token
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          mask_probability: 0.15
          max_seq_length: 20
      masked_region_processor:
        type: masked_region
        params:
          mask_probability: 0.15
          mask_region_probability: 0.90
      answer_processor:
        type: vqa_answer
        params:
          num_answers: 10
          vocab_file: gqa/defaults/vocabs/answers_gqa.txt
          preprocessor:
            type: simple_word
            params: {}
      bbox_processor:
        type: bbox
        params:
          max_length: 36
      attribute_processor:
        type: vocab
        params:
          max_length: 1
          vocab:
            type: random
            vocab_file: gqa/defaults/vocabs/vocabulary_100k.txt
  masked_coco:
    data_dir: ${env.data_dir}/datasets
    depth_first: false
    fast_read: false
    use_images: false
    use_features: true
    use_image_feature_masks: true
    return_features_info: true
    max_features: 36
    use_ocr: false
    use_ocr_info: false
    two_sentence: false
    two_sentence_probability: 0
    false_caption: false
    false_caption_probability: 0.5
    features:
      train:
      - coco/defaults/features/trainval2014.lmdb
      - coco/defaults/features/trainval2014.lmdb
    annotations:
      train:
      - coco/defaults/annotations/imdb_karpathy_train_by_image.npy
      - coco/defaults/annotations/imdb_karpathy_val_by_image.npy
    processors:
      bbox_processor:
        type: bbox
        params:
          max_length: 36
      text_processor:
        type: bert_tokenizer
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          max_seq_length: 20
      attribute_processor:
        type: vocab
        params:
          max_length: 1
          vocab:
            type: random
            vocab_file: coco/defaults/extras/vocabs/vocabulary_100k.txt
      masked_token_processor:
        type: masked_token
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          mask_probability: 0.15
          max_seq_length: 20
      masked_region_processor:
        type: masked_region
        params:
          mask_probability: 0.15
          mask_region_probability: 0.9
